**Vision_Gestures_Interface**


Gesture-based virtual mouse and system control using real-time computer vision. This project leverages MediaPipe, OpenCV, and PyAutoGUI to detect hand gestures and map them to system actions like cursor movement, clicking, volume control, and brightness adjustmentâ€”enabling touchless interaction with your desktop.

ğŸ“Œ Features
ğŸ–ï¸ Real-time Hand Tracking using MediaPipe (30+ FPS)

ğŸ–±ï¸ Mouse Control: Move, click, drag, and scroll via hand gestures

ğŸ”Š Volume & Brightness Control (Windows)

ğŸ§  Gesture Recognition via state-based logic and temporal smoothing

âš™ï¸ Modular Design for gesture-action mapping

ğŸŒ Cross-platform Support: Windows, macOS, Linux (partial features)

ğŸ§  Architecture
mermaid
Copy
Edit
flowchart TD
    A[Camera Feed (OpenCV)] --> B[Hand Detection (MediaPipe)]
    B --> C[Gesture Classification]
    C --> D[Controller Logic]
    D --> E[System Control via PyAutoGUI / OS APIs]
âœ… If Mermaid doesn't render on GitHub, consider replacing this with a static image version.

ğŸš€ Installation
bash
Copy
Edit
# Clone the repository
git clone https://github.com/uayushdubey/Vision_Gestures_Interface.git
cd Vision_Gestures_Interface

# Install dependencies
pip install -r requirements.txt
Optional: Use Poetry
bash
Copy
Edit
poetry install
poetry run python gesture_controller.py
â–¶ï¸ Usage
bash
Copy
Edit
python gesture_controller.py
Use gestures like:

âœŒï¸ Move cursor

ğŸ‘† Click

âœŠ Drag

ğŸ¤˜ Scroll

ğŸ¤ Control volume or brightness

